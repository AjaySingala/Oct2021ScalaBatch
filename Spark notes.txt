Spark:
spark-shell is like an REPL, hive, mongo
Apache Spark is considered to be a "Hadoop (MapReduce") killer".
It is also no, 1 Big Data tool these days.
It is ~100 times faster than MapReduce.
It integrates quite well with Hadoop.
Common mis-belief that Spark is a modified version of Hadoop! Wrong!!!
Features:
	Speed: faster than MR.
	Support for multiple languages: Java, Scala, Python.
	Advaned analytics: SQL queries, Streaming data, ML and Graph algorithms.
MapReduce vs Spark:
	Topic			Hadoop MapReduce		Spark
	Storage			disk only.				in-memory, on disk.
	Operations		Map and Reduce			Map, Reduce, Join, Aggregations...
	Exeuction Model	Batch					Batch, interactive, streaming.
	Programming env	Java					Java, Scala, Python, R.
Spark components:
	Apache Spark Core
		Spark SQL
		Spark Streaming
		MLib (Machine Learning)
		GraphX (graph)
Spark is written in Scala.

RDD - Resilient Distributed Dataset
Fundamental data structure in Spark.
immutable.
Each dataset in an RDD is divided into logical partitions, which may be computed on separate nodes on the cluster.
RDDs can contain objects of any type (for the supported languages: JAva, scala, Python, R), including any user-defined classes.
Resilient: means it is able to withstand /recover very quickly from any lost connection, downtime.

There are 2 ways to create RDDs in Spark:
1. Parallelizing existing collection of data.
2. Directly referencing datasets in an external storage system (for e.g.; AWS S3 or HDFS).

DataSharing in MapReduce is slow.

Shared Variables:
	variables that are shared over different nodes, for computation purpose.
	two types:
		broadcast variables: used to cache values in memory on all the nodes.
		Accumulators: used to perform aggregations (counters, sum, count...)

2 types of operations that can be performed on RDDs:
	Transformation: creates a new RDD after processing.
		Lazy Evaluation methodology.
		The function will be executed ONLY when you perform an action on the RDD.
		map()
		filter()
		flatMap()
		distinct()
		groupByKey()
			data is in (K,V)
			return a dataset of (K, Iterable(V)) pairs.
			"reston", 10
			"dallas", 20
			"reston", 35
			"boston", 12
			....
			"reston", (10, 35)
			"dallas", (20)
			"boston", (12)
		reduceByKey()
		sortByKey()
		cogroup()
		
	Action: do not change the content of the rdd. They only return the content of the rdd and do not generate a new RDD.
		reduce()
		collect()
		count()
		first()
		take(n)
		takeOrdered(n)
		saveAsTextFile(path)
		saveAsSequenceFile(path)
		saveAsObjectFile(path)		only Java and Scala, uses Java Serialization.
		countByKey()
		foreach()
		
val newrdd = rdd.abc()
println(rdd.collect())

rdd.foreach(println)

Spark UI: http://sandbox-hdp.hortonworks.com:4040

val inputfile = sc.textFile("input.txt")

val inputfile = sc.textFile("file:///home/maria_dev/SparkSamples/input.txt")
val t1 = inputfile.flatMap(line => line.split(" "))
t1.foreach(println)


val t2 = inputfile.flatMap(line => line.split(" ")).map(word => (word,1))
val t3 = inputfile.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)
t2.foreach(println)
t3.foreach(println)

val t2 = t1.map(word => (word,1))
val t3 = t2.reduceByKey(_+_)

val t1:RDD[(String)] = inputfile.flatMap(line => line.split(" "))
val t2:RDD[(String,Int)] = inputfile.flatMap(line => line.split(" ")).map(word => (word,1))
val t3:RDD[(String,Int)] = inputfile.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)

Some common Transformations:
map(): iterates over every element in the RDD and performs the given function into a new RDD.
flatMap(): map() will return only 1 element at a time. flatMap() will return a list of elements.
filter(): returns a new RDD that contains only those elements that match the filter criteria (predicate) provided.
reduceByKey(): works on a dataset of (K,V) pairs on the same machine where the same key are combined and result is stored in a new RDD.
sortByKey()= 

val words_sorted = t3.map(a => (a._2, a._1)).sortByKey()

val data = Seq(("Project","A", 1),
    ("Gutenberg’s", "X",3),
    ("Alice’s", "C",5),
    ("Adventures","B", 1)
  )
val rdd=spark.sparkContext.parallelize(data)

val rdd2=rdd.map(f=>{(f._2, (f._1,f._2,f._3))}) 
rdd2.foreach(println)  

val rdd3= rdd2.sortByKey()
OR
val rdd3= rdd2.sortByKey(true,1)
rdd3.foreach(println)
println(rdd3.collect().mkString(","))

// Descending.
val rdd4= rdd2.sortByKey(false)
OR
val rdd4= rdd2.sortByKey(false,1)
rdd4.foreach(println) 
println(rdd4.collect().mkString(","))

Another example of sortByKey:
============================
val data = Seq(("Project","A", 1),
    ("Gutenberg’s", "X",3),
    ("Alice’s", "C",5),
    ("Adventures","B", 1)
  )
val rdd=spark.sparkContext.parallelize(data)

// Convert to Key-Value pairs.
val rdd2=rdd.map(f=>{(f._2, (f._1,f._2,f._3))}) 
rdd2.foreach(println)  

val rdd3= rdd2.sortByKey()
// OR specify no. of partitions.
//val rdd3= rdd2.sortByKey(true,1)
rdd3.foreach(println)
println(rdd3.collect().mkString(","))

// Descending.
val rdd4= rdd2.sortByKey(false)
// OR specify no. of partitions.
// val rdd4= rdd2.sortByKey(false,1)
rdd4.foreach(println) 
println(rdd4.collect().mkString(","))

More actions:
take(n): return "n" number of elements from the RDD. Tries to cut down on the number of partitions it accesses, it  represents biased data. We cannot presume the order of the elements.
top(n): If ordering is present, we can extract the top "n" elements from the RDD. It uses the default ordering of data.

val data = spark.sparkContext.parallelize(Array(('k',5), ('s',3), ('s',4), ('p',7), ('t',8), ('p',5), ('k',6)))
val twoRecs = data.take(2)
twoRecs.foreach(println)

countByKey(): returns how many times each element occurs in an RDD. Returns a hashmap of K,V pairs.

{1,2,2,3,4,5,5,6} 
rdd.countByValue() will give the result as:
{(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)}

countByValue() on a text file:
val file = spark.read.textFile("spark_test.txt").rdd
val result = file.map(line => (line, line.length)).countByValue()
result.foreach(println)

SparkContext and SparkSession:
SparkContext is the entry point to Spark.
Defined in org.apache.spark package.
Default object is "sc".
Use it to create a SparkContext object programmatically.
Use the SparkContext object to create RDDs, Accumulators, Broadcast Variables on the cluster.
Two diff ways to read a file:
1) Using SparkContext: val rdd:RDD[String] = sc.textFile("filename")
2) Using SparkSession: val rdd:RDD[String] = spark.read.textFile("filename").rdd

If you are using the spark-shell, both of these (context and session) are available ready-to-use in variables "sc" and "spark" respectively.
But if you are going to run your spark code from outside of spark-shell (using spark-submit), you have to create instances of both.
For e.g.;
val spark:SparkSession = SparkSession.builder()
  .master("local[3]")
  .appName("some name")
  .getOrCreate()

val sc = spark.sparkContext

SparkSession was introduced in Spark 2.0.
It is the entry point for the underlying functionality in Spark to create DataFrames, DataSets (to use SparkSQL) and even RDDs.
Default object is "spark".
Defined in org.apache.spark.sql package.
SparkSession can be used to work with different APIs:
	Spark Context
	SQL Context
	Streaming Context
	Hive Context
	
val spark:SparkSession = SparkSession.builder()
  .master("local[3]")
  .appName("ajaysingala.com")
  .getOrCreate()

.master(...):
"local"							=>	to run locally.
"local[4]"						=> run locally with 4 cores.
"spark://master:7077"			=> run on a single-node cluster.
"acme.sparkcluster.org:7077"	=> specific location of the spark server.

.appname("...")					=> set a application name that will appear on Spark Web UI.
.getOrCreate()					=> Gets an existing instance (if any) or creates a new one.
.config("config-key", value)	=> set configuration values.
	.config("spark.driver.memory", "512m")
	
RDD Transformations:
	DAG: Directed Acyclic Graph
	RDD Transformations and create a new RDD from another, is known as "RDD Lineage".
		a.k.a. RDD operator graph or RDD dependency graph.
	RDD Transformations are lazy operations: none of the transformations get executed until you execute an action on an RDD.
	RDDs are immutable.
	Two types of transformations:
		Narrow Transformation:
			all the elements that are required to perform computation on the records in a single partition live in the single partition of the RDD.
			A limited subset of partition is used to calculate the result.
			Examples of narrow transformations: map() filter() flatMap()
			These compute the data that live on a single partition. Meaning, there will not be any data movement between partitions to execute narrow transformations.
			
		Wide Transformation:
			All the elements that are required to compute the records in a single partition may live in many partitions of the parent RDD.
			examples: groupByKey() reduceByKey()
			These transformations compute data that live on many partitions, meaning there will be data movement between partitions to exeucte wide transformations.
			a.k.a. Shuffle Transformations.
			
Shared Variables:
	variables that are shared over different nodes, for computation purpose.
	two types:
		broadcast variables: used to cache values in memory on all the nodes.
			val broadcastVar = sc.broadcast(Array(1,2,3))
		Accumulators: used to perform aggregations (counters, sum, count...)
			accumulators are basically an initial value that can then be used by processes for further computation.
			Spark natively supports accumulators of numeric types.
			to create: sc.accumulator(initial value)
			for e.g.; sc.accumulator(0)
			val accnum = sc.accumulator(0)
			val result = sc.parallelize(Array(1,2,3,4)).foreach(x => accnum += x)
			
fold()
	a function to calculate min, max, total elements.
	takes 2 params:
		zeroValue: initial value (accumulator). Used to initialize the accumulator for each partition.
			0 for numbers. Nil for collections.
		op: operator used to both accumulate the results within a partition and combine the results from all the partitions.
		
	The param zeroValue is the initial value for the accumulated result of each partition for the op operator, and also the initial value for the combine results from different partitions for the op operator.
		
	is similar to reduce(), except that it takes an initial value to begin with (zeroValue)
	
val rddMarks = spark.sparkContext.parallelize(List(("Maths", 80), ("Science", 75), ("English", 82)))
val additionalMarks = ("Extra", 4)
val totalMarks = rddMarks.fold(additionalMarks) ((acc, marks) => { ("Total", acc._2 + marks._2) })
println(totalMarks)

val listRdd = spark.sparkContext.parallelize(List(1,2,3,4,5,3,2))
(2+1+2) + (2+3+4) + (2+5+3) + (2+2) + 2	// (extra one for combining results)


val rdd1 = sc.parallelize(List(1,2,3,4,5), 3)
rdd1.fold(5)(_ + _)
(5 + 1) + (5 + 2 + 3) + (5 + 4 + 5) + 5 // (extra one for combining results)

Deploy Modes:
cluster:
	the driver runs on one of the worker nodes, and this node is the one that shows up Spark Web UI.
	used to run production jobs.
	spark-submit --deploy-mode cluster
	the driver program does not run on the machine where it has been submitted, but it runs on the cluster as a sub-process of the Applicaiton Master (YARN).
	If the driver program fails, it is re-instantiated automatically by the App Master.
client:
	the driver runs locally from where you submit the job, using spark-submit.
	majorly used for debugging or interactive purpose.
	Driver will run locally, but then the tasks run on the cluster worker nodes.
	spark-submit --deploy-mode client
	if the driver program fails, the entire job will fail.
	
Reading files:
wholeTextFiles(): Read tet files into RDD of Tuple.
val rddWhole = spark.sparkContext.wholeTextFiles("file:///home/maria_dev/SparkSamples/resources/csv/*")
type is RDD[(String, String)]
	where:	the key is the path of the file.
			the value is the contents of the file.
	
union:
val rdd1 =
spark.sparkContext.parallelize(Seq((1,"jan",2016),(3,"nov",2014),(16,"feb",2014)))
val rdd2 =
 spark.sparkContext.parallelize(Seq((5,"dec",2014),(17,"sep",2015)))
val rdd3 =
 spark.sparkContext.parallelize(Seq((6,"dec",2011),(16,"may",2015)))
val rddUnion = rdd1.union(rdd2).union(rdd3)
rddUnion.foreach(println)


intersection:
val is1 = sc.parallelize(List("c","c","p","m","t"))
val is2 = sc.parallelize(List("c","m","k"))
val result = is1.intersection(is2)
result.foreach{println}

distinct:
val rdd1 =
spark.sparkContext.parallelize(Seq((1,"jan",2016),(3,"nov",2014),(16,"feb",2014),(3,"nov",2014)))
val result = rdd1.distinct()
println(result.collect().mkString(", "))

groupByKey:
val data =
spark.sparkContext.parallelize(Array(('k',5),('s',3),('s',4),('p',7),('p',5),('t',8),('k',6)),3)

val group = data.groupByKey().collect()
group.foreach(println)

join:
val data =
 spark.sparkContext.parallelize(Array(('A',1),('b',2),('c',3)))
val data2 =
spark.sparkContext.parallelize(Array(('A',4),('A',6),('b',7),('c',3),('c',8)))
val result = data.join(data2)
println(result.collect().mkString(","))

repartition:
val rdd = spark.sparkContext.parallelize(Range(0,20))
println("From local : "+rdd.partitions.size)

val rdd1 = spark.sparkContext.parallelize(Range(0,25), 6)
println("parallelize : "+rdd1.partitions.size)

val rddFromFile = spark.sparkContext.textFile("file:///home/maria_dev/SparkSamples/resources/spark_test.txt",10)
println("TextFile : "+rddFromFile.partitions.size)

rdd1.saveAsTextFile("file:///tmp/partition")

val rdd2 = rdd1.repartition(4)
println("Repartition size : "+rdd2.partitions.size)
rdd2.saveAsTextFile("file:///tmp/re-partition")

coalesce:
val rdd3 = rdd1.coalesce(4)
println("Repartition size : "+rdd3.partitions.size)
rdd3.saveAsTextFile("file:///tmp/coalesce")

cogroup:
val data1 = sc.parallelize(Seq(("A",1),("B",2),("C",3)))  
data1.collect  
val data2 = sc.parallelize(Seq(("B",4),("E",5)))  
data2.collect  
val cogroupresult = data1.cogroup(data2)  
cogroupresult.collect
cogroupfunc.foreach(println)
